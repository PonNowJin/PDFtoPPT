[
    [
        {
            "page": 1,
            "bbox": [
                1294,
                978,
                2350,
                1391
            ],
            "caption": "Fig. 2: An example self-attention block used in the vision damain [20] Cuixzran tho innit caniianco anf imaacoa faatirac tha",
            "image_name": "crop_2_image_1.png"
        }
    ],
    [
        {
            "page": 1,
            "bbox": [
                156,
                127,
                2395,
                898
            ],
            "caption": "Fig. 1: Statistics on the number of times keywords such as BERT, Self-Attention, and Transformers appear in the titles of Peer- reviewed and arXiv papers over the past few years (in Computer Vision and Machine Learning). The plots show consistent growth in recent literature. This survey covers recent progress on Transformers in the computer vision domain. eivra and acanaraligahlo ronrocantatnnec that modal rich rala. wk",
            "image_name": "crop_2_image_2.png"
        }
    ],
    [
        {
            "page": 1,
            "bbox": [
                227,
                222,
                912,
                735
            ],
            "caption": "",
            "image_name": "crop_2_image_3.png"
        }
    ],
    [
        {
            "page": 2,
            "bbox": [
                176,
                153,
                2373,
                1062
            ],
            "caption": "Fig. 3: Architecture of the Transformer Model [1]. The model was first developed for the language translation task where an input one language is required to be converted to the output sequence in another language. The Transformer encoder Scaled-Dot Product Attention sequence in ~ 7h \\",
            "image_name": "crop_3_image_1.png"
        }
    ],
    [
        {
            "page": 4,
            "bbox": [
                251,
                135,
                2378,
                929
            ],
            "caption": "",
            "image_name": "crop_5_image_1.png"
        }
    ],
    [
        {
            "page": 4,
            "bbox": [
                1284,
                1143,
                2359,
                1600
            ],
            "caption": "Fig. 5: Comparison of two different self-attention approaches: Non-local self-attention block [70] and Criss-cross self-attention module [72]. Figure is from [72].",
            "image_name": "crop_5_image_2.png"
        }
    ],
    [
        {
            "page": 4,
            "bbox": [
                306,
                153,
                1606,
                921
            ],
            "caption": "",
            "image_name": "crop_5_image_3.png"
        }
    ],
    [
        {
            "page": 6,
            "bbox": [
                180,
                156,
                1253,
                1008
            ],
            "caption": "Fig. 6: An overview of Vision Transformer (on the Jeff) and the details of Transformer encoder (on the right). The architecture resembles Transformers used in the NLP domain and the image patches are simply fed to the model after flattening. After training, the feature obtained from the first token position is used for classification. Image obtained from [11]. out useful representations for input images. A distillation",
            "image_name": "crop_7_image_1.png"
        }
    ],
    [
        {
            "page": 8,
            "bbox": [
                1280,
                164,
                2377,
                702
            ],
            "caption": "Fig. 8: Axial attention module [133] that sequentially applies multi-head axial attention operations along height and width axes. Image from [133]. requires a large number of training epochs to tune attention",
            "image_name": "crop_9_image_1.png"
        }
    ],
    [
        {
            "page": 8,
            "bbox": [
                171,
                173,
                1263,
                835
            ],
            "caption": "Fig. 7: Detection Transformer (DETR) [13] treats the object detection task as a set prediction problem and uses the Trans- former network to encode relationships between set elements. A bipartite set loss is used to uniquely match the box predic- tions with the ground-truth boxes (shown on the right two columns). In case of no match, a ‘no object’ class prediction is selected. Its simple design with minimal problem-specific modifications can beat a carefully built and popular Faster R-",
            "image_name": "crop_9_image_2.png"
        }
    ],
    [
        {
            "page": 9,
            "bbox": [
                1282,
                119,
                2358,
                1675
            ],
            "caption": "Fig. 9: (a) Self-attention block in Image Transformer [142]. Given one channel for a pixel q, the block attends to the mem- ory of previous synthesized pixels (m:), followed by a feed- forward sub-network. Positional encodings p; are added in the first layer. (b) The operation performed in Local Self-Attention (example of a 2D case is shown). The image is partitioned into a grid of spatial blocks known as query blocks. In the self- attention operation, each pixel in a query block attends to all",
            "image_name": "crop_10_image_1.png"
        }
    ],
    [
        {
            "page": 11,
            "bbox": [
                164,
                163,
                2374,
                746
            ],
            "caption": "Fig. 10: Images generated by DALL-E [20] from the following text prompts. (a) An armchair in the shape of an avocado. (b) A photo of San Francisco’s golden gate bridge. Given a part of the image (in green box), DALL-E performs the image completion. (c) An emoji of a baby penguin wearing a blue hat, red gloves, green shirt, and yellow pants. (d) An extreme close-up view of a capybara sitting in a field. (e) A cross-section view of a pomegranate. (f) A penguin made of watermelon. (g) The exact same cat on the top as a sketch on the bottom. infeasible to develop Transformer model that can operate on task, can provide significant performance gains over the",
            "image_name": "crop_12_image_1.png"
        }
    ],
    [
        {
            "page": 12,
            "bbox": [
                293,
                194,
                1240,
                1464
            ],
            "caption": "ition image. Figure is from [16]. ngle image super-resolution problem in which only",
            "image_name": "crop_13_image_1.png"
        }
    ],
    [
        {
            "page": 13,
            "bbox": [
                210,
                207,
                2384,
                1657
            ],
            "caption": "",
            "image_name": "crop_14_image_1.png"
        }
    ],
    [
        {
            "page": 13,
            "bbox": [
                220,
                199,
                2408,
                1015
            ],
            "caption": "",
            "image_name": "crop_14_image_2.png"
        }
    ],
    [
        {
            "page": 13,
            "bbox": [
                163,
                893,
                2418,
                1651
            ],
            "caption": "Fig. 12: An overview of Transformer models used for multi-modal tasks in computer vision. The Transformer designs in this cateocoary can he oroined into cinole-ctream (TINTTER [42] OSCAR [44] VideoRERT [171] TInicoder-VT [12801 ViewalBERT [62] and",
            "image_name": "crop_14_image_3.png"
        }
    ],
    [
        {
            "page": 15,
            "bbox": [
                1282,
                172,
                2358,
                997
            ],
            "caption": "Fig. 13: Visualized tokens (Vokens) [191]: A language model is visually supervised using closely related images that leads to better feature representations from the pretrained model. Figure from [191].",
            "image_name": "crop_16_image_1.png"
        }
    ],
    [
        {
            "page": 17,
            "bbox": [
                1296,
                167,
                2365,
                1558
            ],
            "caption": "Fig. 14: Spatial/Temporal Attention for Skeleton Data Repre- sentations. Relationships between body-joints and inter-frame dependencies are modeled using two dedicated self-attention modules. Figure is from [216]. backbone CNN on a collection of video frames. An encoder",
            "image_name": "crop_18_image_1.png"
        }
    ],
    [
        {
            "page": 18,
            "bbox": [
                179,
                176,
                1264,
                1011
            ],
            "caption": "Fig. 15: An overview of FEAT [26]. Compared to the con- ventional instance embedding methods in FSL that keep the embedding function same for all tasks (a), FEAT uses a set- to-set function to adapt the embedding function to each FSL task (b). It evaluates several set-to-set functions and found the Transformer module to be the most suitable choice for FSL. Figure from [26].",
            "image_name": "crop_19_image_1.png"
        }
    ],
    [
        {
            "page": 20,
            "bbox": [
                332,
                352,
                2270,
                2743
            ],
            "caption": "bedding suitable for the Encoder. DeiT [12] Transformer as s student while CNN as 2D Image Class labels Cross-entropy, a teacher, Distillation tokens to produce Distillation loss estimated labels from teacher, Attention based on between class and distillation tokens. KL-divergence CLIP [195] Jointly train image and text encoders on 2D Images & texts Image-text Symmetric image-text pairs, to maximize similarity of pairs cross-entropy valid pairs and minimize otherwise",
            "image_name": "crop_21_table_1.png"
        }
    ],
    [
        {
            "page": 20,
            "bbox": [
                338,
                362,
                2238,
                1362
            ],
            "caption": "bedding suitable for the Encoder. DeiT [12] Transformer as s student while CNN as 2D Image Class labels Cross-entropy, a teacher, Distillation tokens to produce Distillation loss estimated labels from teacher, Attention based on between class and distillation tokens. KL-divergence CLIP [195] Jointly train image and text encoders on 2D Images & texts Image-text Symmetric image-text pairs, to maximize similarity of pairs cross-entropy valid pairs and minimize otherwise",
            "image_name": "crop_21_table_2.png"
        }
    ],
    [
        {
            "page": 21,
            "bbox": [
                355,
                180,
                2182,
                2840
            ],
            "caption": "",
            "image_name": "crop_22_table_1.png"
        }
    ],
    [
        {
            "page": 21,
            "bbox": [
                359,
                228,
                2130,
                1696
            ],
            "caption": "",
            "image_name": "crop_22_table_2.png"
        }
    ],
    [
        {
            "page": 22,
            "bbox": [
                191,
                1055,
                2387,
                2296
            ],
            "caption": "Fig. 16: Mesh Transformer architecture. The joint and vertex queries are appended with positional embeddings and passed through multiple self-attention layers to jointly regress 3D coordinates of joints and mesh vertices. Figure is from [45]. Method #Param (M) “GFLOPs Top-1 Acc (%) Method #Param (M) GFLOPs  Top-1 Acc (%) ResNet18 [67]« 11.7 1.8 69.8 ResNet101 [67] « 44.7 7.9 774 EfficientNet-B3 [87]« 12.0 1.8 81.6 ResNeXt101-32x4d [244] 44.2 8.0 78.8 DeiT-T [12] 5.7 1.3 72.2 RegNetY-8G [86]« 39.0 8.0 81.7 T2T-ViT;-7 [35] 5.0 1.3 71.7 EfficientNet-B5 [87] « 30.0 9.9 83.6 LocalViT-T [107] 5.9 1.3 74.8 CvT-21 [96] 32.0 7.1 82.5",
            "image_name": "crop_23_table_1.png"
        }
    ],
    [
        {
            "page": 22,
            "bbox": [
                205,
                197,
                2337,
                1084
            ],
            "caption": "Fig. 16: Mesh Transformer architecture. The joint and vertex queries are appended with positional embeddings and passex through multiple self-attention layers to jointly regress 3D coordinates of joints and mesh vertices. Figure is from [45]. Method #Param (M) “GFLOPs Top-1 Acc (%) Method #Param (M) GFLOPs  Top-1 Acc (%) mn ATsao Trt anaes an ENO 人 AAP arn el",
            "image_name": "crop_23_image_2.png"
        }
    ],
    [
        {
            "page": 22,
            "bbox": [
                202,
                200,
                1289,
                1082
            ],
            "caption": "Fig. 16: Mesh Transformer architecture. The joint and vertex q through multiple self-attention layers to jointly regress 3D coordi Method #Param (M) “GFLOPs Top-1 Acc (%) ~~ 0hUt oan poet aae aan CN A",
            "image_name": "crop_23_image_3.png"
        }
    ]
]